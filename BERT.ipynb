{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMZ0pFdork9QYYkEa4CLkvN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skipperuzumaki/Machiene_Learning/blob/master/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB6wCdAiYHh1"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b24ZLLI-W8zt"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "%tensorflow_version 1.x\n",
        "import sys\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_32KP7xSXLBV"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTj7zDuJXNRG"
      },
      "source": [
        "repo = 'model_repo'\n",
        "with zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(repo)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYugk3IvXYSc"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/run_squad.py \n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoZLu9-kXbQG"
      },
      "source": [
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "#We will use the most basic of all of them\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12'\n",
        "BERT_PRETRAINED_DIR = f'{repo}/uncased_L-12_H-768_A-12'\n",
        "OUTPUT_DIR = f'{repo}/outputs'\n",
        "print(f'***** Model output directory: {OUTPUT_DIR} *****')\n",
        "print(f'***** BERT pretrained directory: {BERT_PRETRAINED_DIR} *****')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heWvaXvYXiER"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rxmQuI3XlOv"
      },
      "source": [
        "def is_whitespace(c):\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "        return True\n",
        "    return False\n",
        "def get_squad_attributes(row):\n",
        "    paragraph_text = row['Text']\n",
        "    is_impossible = row['is_impossible'] \n",
        "    doc_tokens = []\n",
        "    char_to_word_offset = []\n",
        "    prev_is_whitespace = True\n",
        "    for c in paragraph_text:\n",
        "        if is_whitespace(c):\n",
        "            prev_is_whitespace = True\n",
        "        else:\n",
        "            if prev_is_whitespace:\n",
        "                doc_tokens.append(c)\n",
        "            else:\n",
        "                doc_tokens[-1] += c\n",
        "            prev_is_whitespace = False\n",
        "        char_to_word_offset.append(len(doc_tokens) - 1)\n",
        "        \n",
        "    if not is_impossible:\n",
        "        if row['A-coref']:\n",
        "            orig_answer_text = row['A']\n",
        "            answer_offset = row['A-offset']\n",
        "        else:\n",
        "            orig_answer_text = row['B']\n",
        "            answer_offset = row['B-offset']\n",
        "            \n",
        "        answer_length = len(orig_answer_text)\n",
        "        start_position = char_to_word_offset[answer_offset]\n",
        "        end_position = char_to_word_offset[answer_offset + answer_length -\n",
        "                                       1]\n",
        "        # Only add answers where the text can be exactly recovered from the\n",
        "        # document. If this CAN'T happen it's likely due to weird Unicode\n",
        "        # stuff so we will just skip the example.\n",
        "        #\n",
        "        # Note that this means for training mode, every example is NOT\n",
        "        # guaranteed to be preserved.\n",
        "        actual_text = \" \".join(\n",
        "            doc_tokens[start_position:(end_position + 1)])\n",
        "        cleaned_answer_text = \" \".join(\n",
        "            tokenization.whitespace_tokenize(orig_answer_text))\n",
        "\n",
        "        if actual_text.find(cleaned_answer_text) == -1:\n",
        "            tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
        "                                 actual_text, cleaned_answer_text)\n",
        "#             continue\n",
        "    else:\n",
        "        start_position = -1\n",
        "        end_position = -1\n",
        "        orig_answer_text = \"\"\n",
        "    return pd.Series({'doc_tokens':doc_tokens, \n",
        "                      'char_to_word_offset':char_to_word_offset,\n",
        "                      'orig_answer_text':orig_answer_text,\n",
        "                      'start_position': start_position,\n",
        "                      'end_position': end_position,\n",
        "                     })"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "GZ1P-dGFXqOk",
        "outputId": "ff3ed231-6be4-4540-d2d5-c31bc6710e08"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import run_squad\n",
        "import modeling\n",
        "import optimization\n",
        "import tokenization\n",
        "import tensorflow as tf\n",
        "\n",
        "train_df =  pd.read_csv('gap-development.tsv', sep='\\t')\n",
        "train_df = train_df.sample(2000)\n",
        "print(train_df.columns)\n",
        "# add is_impossible\n",
        "train_df['is_impossible'] = ~train_df['A-coref'] & ~train_df['B-coref']\n",
        "# add doc tokens\n",
        "# add start word number of answer text\n",
        "# add end word number of answer text\n",
        "train_df_full = train_df.merge(train_df.apply(lambda row: get_squad_attributes(row), axis=1),\n",
        "                             left_index=True, right_index=True)\n",
        "\n",
        "train, test = train_test_split(train_df_full, test_size = 0.3, random_state=42)\n",
        "\n",
        "train.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Index(['ID', 'Text', 'Pronoun', 'Pronoun-offset', 'A', 'A-offset', 'A-coref',\n",
            "       'B', 'B-offset', 'B-coref', 'URL'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Text</th>\n",
              "      <th>Pronoun</th>\n",
              "      <th>Pronoun-offset</th>\n",
              "      <th>A</th>\n",
              "      <th>A-offset</th>\n",
              "      <th>A-coref</th>\n",
              "      <th>B</th>\n",
              "      <th>B-offset</th>\n",
              "      <th>B-coref</th>\n",
              "      <th>URL</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>doc_tokens</th>\n",
              "      <th>char_to_word_offset</th>\n",
              "      <th>orig_answer_text</th>\n",
              "      <th>start_position</th>\n",
              "      <th>end_position</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>development-184</td>\n",
              "      <td>In 2010 Ella Kabambe was not the official Miss...</td>\n",
              "      <td>her</td>\n",
              "      <td>304</td>\n",
              "      <td>Susan Mtegha</td>\n",
              "      <td>166</td>\n",
              "      <td>True</td>\n",
              "      <td>Collette Lochore</td>\n",
              "      <td>204</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Miss_Malawi</td>\n",
              "      <td>False</td>\n",
              "      <td>[In, 2010, Ella, Kabambe, was, not, the, offic...</td>\n",
              "      <td>[0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, ...</td>\n",
              "      <td>Susan Mtegha</td>\n",
              "      <td>29</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1894</th>\n",
              "      <td>development-1895</td>\n",
              "      <td>As children, the Roberts siblings attended Jef...</td>\n",
              "      <td>she</td>\n",
              "      <td>310</td>\n",
              "      <td>Susan Kingman</td>\n",
              "      <td>250</td>\n",
              "      <td>False</td>\n",
              "      <td>Roberts</td>\n",
              "      <td>283</td>\n",
              "      <td>True</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Emma_Roberts_(art...</td>\n",
              "      <td>False</td>\n",
              "      <td>[As, children,, the, Roberts, siblings, attend...</td>\n",
              "      <td>[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, ...</td>\n",
              "      <td>Roberts</td>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>873</th>\n",
              "      <td>development-874</td>\n",
              "      <td>In 2002, Carr and Pournelle were honored with ...</td>\n",
              "      <td>his</td>\n",
              "      <td>319</td>\n",
              "      <td>Carr</td>\n",
              "      <td>152</td>\n",
              "      <td>False</td>\n",
              "      <td>Wolfgang Diehr</td>\n",
              "      <td>199</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/John_F._Carr</td>\n",
              "      <td>True</td>\n",
              "      <td>[In, 2002,, Carr, and, Pournelle, were, honore...</td>\n",
              "      <td>[0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, ...</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1022</th>\n",
              "      <td>development-1023</td>\n",
              "      <td>She died of pneumonia on Sunday, October 19, 1...</td>\n",
              "      <td>her</td>\n",
              "      <td>241</td>\n",
              "      <td>Betty Jane Butler</td>\n",
              "      <td>160</td>\n",
              "      <td>False</td>\n",
              "      <td>May Wilson</td>\n",
              "      <td>252</td>\n",
              "      <td>True</td>\n",
              "      <td>http://en.wikipedia.org/wiki/May_Wilson</td>\n",
              "      <td>False</td>\n",
              "      <td>[She, died, of, pneumonia, on, Sunday,, Octobe...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, ...</td>\n",
              "      <td>May Wilson</td>\n",
              "      <td>42</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1935</th>\n",
              "      <td>development-1936</td>\n",
              "      <td>He also wrote and produced several songs for i...</td>\n",
              "      <td>her</td>\n",
              "      <td>266</td>\n",
              "      <td>Estelle</td>\n",
              "      <td>162</td>\n",
              "      <td>False</td>\n",
              "      <td>Nelly Furtado</td>\n",
              "      <td>190</td>\n",
              "      <td>True</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Di_Genius</td>\n",
              "      <td>False</td>\n",
              "      <td>[He, also, wrote, and, produced, several, song...</td>\n",
              "      <td>[0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, ...</td>\n",
              "      <td>Nelly Furtado</td>\n",
              "      <td>28</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    ID  ... end_position\n",
              "183    development-184  ...           30\n",
              "1894  development-1895  ...           42\n",
              "873    development-874  ...           -1\n",
              "1022  development-1023  ...           43\n",
              "1935  development-1936  ...           29\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxXsVMOvXtky",
        "outputId": "0b1cf8cb-3c4f-45c5-ada6-726441f18e11"
      },
      "source": [
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default GPU Device: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N08vwePZGfQ"
      },
      "source": [
        "def create_squad_example(row):\n",
        "    return run_squad.SquadExample(\n",
        "        qas_id=row['ID'],\n",
        "        question_text=\"Which noun does \"+ row['Pronoun'] + \" refer to?\",\n",
        "        doc_tokens=row['doc_tokens'],\n",
        "        orig_answer_text=row['orig_answer_text'],\n",
        "        start_position=row['start_position'],\n",
        "        end_position=row['end_position'],\n",
        "        is_impossible=row['is_impossible']\n",
        "    )\n",
        "\n",
        "\n",
        "def create_examples(rows, set_type):\n",
        "#Generate data for the BERT model\n",
        "    guid = f'{set_type}'\n",
        "    examples = []\n",
        "    if guid == 'train':\n",
        "        for row in rows:\n",
        "            examples.append(run_squad.SquadExample(\n",
        "                    qas_id=row['ID'], # ID\n",
        "                    question_text=question_text, #TBD\n",
        "                    doc_tokens=doc_tokens, #doc_tokens\n",
        "                    orig_answer_text=orig_answer_text, # \n",
        "                    start_position=start_position,\n",
        "                    end_position=end_position,\n",
        "                    is_impossible=is_impossible)\n",
        "                           )\n",
        "    else:\n",
        "        for line in lines:\n",
        "            text_a = line\n",
        "            label = '0'\n",
        "            examples.append(\n",
        "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "    return examples\n",
        "\n",
        "# Model Hyper Parameters\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "WARMUP_PROPORTION = 0.1\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 1000 #if you wish to finetune a model on a larger dataset, use larger interval\n",
        "# each checpoint weights about 1,5gb\n",
        "ITERATIONS_PER_LOOP = 1000\n",
        "NUM_TPU_CORES = 8\n",
        "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\n",
        "train_examples = train.apply(lambda row:create_squad_example(row), axis=1)\n",
        "\n",
        "tpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n",
        "#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "num_train_steps = int(\n",
        "    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "model_fn = run_squad.model_fn_builder(\n",
        "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "    init_checkpoint=INIT_CHECKPOINT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n",
        "    use_one_hot_embeddings=True)\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    predict_batch_size=EVAL_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bqm81IPZPjg"
      },
      "source": [
        "\"\"\"\n",
        "Note: You might see a message 'Running train on CPU'. \n",
        "This really just means that it's running on something other than a Cloud TPU, which includes a GPU.\n",
        "\"\"\"\n",
        "!mkdir model_repo/outputs\n",
        "\n",
        "# Train the model.\n",
        "print('Please wait...')\n",
        "train_writer = run_squad.FeatureWriter(\n",
        "        filename=os.path.join(OUTPUT_DIR, \"train.tf_record\"),\n",
        "        is_training=True)\n",
        "train_features = run_squad.convert_examples_to_features(\n",
        "    train_examples, tokenizer, MAX_SEQ_LENGTH, 128, 64, True, train_writer.process_feature)\n",
        "train_writer.close()\n",
        "print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "print('  Num examples = {}'.format(len(train_examples)))\n",
        "print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "train_input_fn = run_squad.input_fn_builder(\n",
        "    input_file=train_writer.filename,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=True)\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print('***** Finished training at {} *****'.format(datetime.datetime.now()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrLb4jmoZffr"
      },
      "source": [
        "def create_test_squad_example(row):\n",
        "    return run_squad.SquadExample(\n",
        "        qas_id=row['ID'],\n",
        "        question_text=\"Which noun does \"+ row['Pronoun'] + \" refer to?\",\n",
        "        doc_tokens=row['doc_tokens'],\n",
        "        orig_answer_text=\"\",\n",
        "        start_position=-1,\n",
        "        end_position=-1,\n",
        "        is_impossible=row['is_impossible']\n",
        "    )\n",
        "\n",
        "eval_examples = test.apply(lambda row:create_test_squad_example(row), axis=1)\n",
        "\n",
        "eval_writer = run_squad.FeatureWriter(\n",
        "    filename=os.path.join(OUTPUT_DIR, \"eval.tf_record\"),\n",
        "    is_training=False)\n",
        "\n",
        "eval_features = []\n",
        "\n",
        "def append_feature(feature):\n",
        "    eval_features.append(feature)\n",
        "    eval_writer.process_feature(feature)\n",
        "    \n",
        "run_squad.convert_examples_to_features(\n",
        "        examples=eval_examples,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        doc_stride=128,\n",
        "        max_query_length=64,\n",
        "        is_training=False,\n",
        "        output_fn=append_feature)\n",
        "\n",
        "eval_writer.close()\n",
        "\n",
        "tf.logging.info(\"***** Running predictions *****\")\n",
        "tf.logging.info(\"  Num orig examples = %d\", len(eval_examples))\n",
        "tf.logging.info(\"  Num features = %d\", len(eval_features))\n",
        "\n",
        "predict_input_fn = run_squad.input_fn_builder(\n",
        "    input_file=eval_writer.filename,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r78ottA9bARY"
      },
      "source": [
        "all_results = []\n",
        "for result in estimator.predict(predict_input_fn, yield_single_examples=True):\n",
        "    if len(all_results) % 1000 == 0:\n",
        "        tf.logging.info(\"Processing example: %d\" % (len(all_results)))\n",
        "    unique_id = int(result[\"unique_ids\"])\n",
        "    start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
        "    end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
        "    all_results.append(\n",
        "          run_squad.RawResult(\n",
        "              unique_id=unique_id,\n",
        "              start_logits=start_logits,\n",
        "              end_logits=end_logits))\n",
        "\n",
        "output_prediction_file = os.path.join(OUTPUT_DIR, \"predictions.json\")\n",
        "output_nbest_file = os.path.join(OUTPUT_DIR, \"nbest_predictions.json\")\n",
        "output_null_log_odds_file = os.path.join(OUTPUT_DIR, \"null_odds.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0hUhA0ebm83"
      },
      "source": [
        "def write_predictions(all_examples, all_features, all_results, n_best_size,\n",
        "                      max_answer_length, do_lower_case, output_prediction_file,\n",
        "                      output_nbest_file, output_null_log_odds_file):\n",
        "  \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
        "  tf.logging.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
        "  tf.logging.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
        "\n",
        "  example_index_to_features = collections.defaultdict(list)\n",
        "  for feature in all_features:\n",
        "    example_index_to_features[feature.example_index].append(feature)\n",
        "\n",
        "  unique_id_to_result = {}\n",
        "  for result in all_results:\n",
        "    unique_id_to_result[result.unique_id] = result\n",
        "\n",
        "  _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "      \"PrelimPrediction\",\n",
        "      [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "  all_predictions = collections.OrderedDict()\n",
        "  all_nbest_json = collections.OrderedDict()\n",
        "  scores_diff_json = collections.OrderedDict()\n",
        "\n",
        "  for (example_index, example) in enumerate(all_examples):\n",
        "    features = example_index_to_features[example_index]\n",
        "\n",
        "    prelim_predictions = []\n",
        "    # keep track of the minimum score of null start+end of position 0\n",
        "    score_null = 1000000  # large and positive\n",
        "    min_null_feature_index = 0  # the paragraph slice with min mull score\n",
        "    null_start_logit = 0  # the start logit at the slice with min null score\n",
        "    null_end_logit = 0  # the end logit at the slice with min null score\n",
        "    for (feature_index, feature) in enumerate(features):\n",
        "      result = unique_id_to_result[feature.unique_id]\n",
        "      start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
        "      end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
        "      # if we could have irrelevant answers, get the min score of irrelevant\n",
        "      for start_index in start_indexes:\n",
        "        for end_index in end_indexes:\n",
        "          # We could hypothetically create invalid predictions, e.g., predict\n",
        "          # that the start of the span is in the question. We throw out all\n",
        "          # invalid predictions.\n",
        "          if start_index >= len(feature.tokens):\n",
        "            continue\n",
        "          if end_index >= len(feature.tokens):\n",
        "            continue\n",
        "          if start_index not in feature.token_to_orig_map:\n",
        "            continue\n",
        "          if end_index not in feature.token_to_orig_map:\n",
        "            continue\n",
        "          if not feature.token_is_max_context.get(start_index, False):\n",
        "            continue\n",
        "          if end_index < start_index:\n",
        "            continue\n",
        "          length = end_index - start_index + 1\n",
        "          if length > max_answer_length:\n",
        "            continue\n",
        "          prelim_predictions.append(\n",
        "              _PrelimPrediction(\n",
        "                  feature_index=feature_index,\n",
        "                  start_index=start_index,\n",
        "                  end_index=end_index,\n",
        "                  start_logit=result.start_logits[start_index],\n",
        "                  end_logit=result.end_logits[end_index]))\n",
        "\n",
        "    prelim_predictions = sorted(\n",
        "        prelim_predictions,\n",
        "        key=lambda x: (x.start_logit + x.end_logit),\n",
        "        reverse=True)\n",
        "\n",
        "    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "    seen_predictions = {}\n",
        "    nbest = []\n",
        "    for pred in prelim_predictions:\n",
        "      if len(nbest) >= n_best_size:\n",
        "        break\n",
        "      feature = features[pred.feature_index]\n",
        "      if pred.start_index > 0:  # this is a non-null prediction\n",
        "        tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
        "        orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "        orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "        orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
        "        tok_text = \" \".join(tok_tokens)\n",
        "\n",
        "        # De-tokenize WordPieces that have been split off.\n",
        "        tok_text = tok_text.replace(\" ##\", \"\")\n",
        "        tok_text = tok_text.replace(\"##\", \"\")\n",
        "\n",
        "        # Clean whitespace\n",
        "        tok_text = tok_text.strip()\n",
        "        tok_text = \" \".join(tok_text.split())\n",
        "        orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "        final_text = run_squad.get_final_text(tok_text, orig_text, do_lower_case)\n",
        "        if final_text in seen_predictions:\n",
        "          continue\n",
        "\n",
        "        seen_predictions[final_text] = True\n",
        "      else:\n",
        "        final_text = \"\"\n",
        "        seen_predictions[final_text] = True\n",
        "\n",
        "      nbest.append(\n",
        "          _NbestPrediction(\n",
        "              text=final_text,\n",
        "              start_logit=pred.start_logit,\n",
        "              end_logit=pred.end_logit))\n",
        "\n",
        "    # In very rare edge cases we could have no valid predictions. So we\n",
        "    # just create a nonce prediction in this case to avoid failure.\n",
        "    if not nbest:\n",
        "      nbest.append(\n",
        "          _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
        "\n",
        "    assert len(nbest) >= 1\n",
        "\n",
        "    total_scores = []\n",
        "    best_non_null_entry = None\n",
        "    for entry in nbest:\n",
        "      total_scores.append(entry.start_logit + entry.end_logit)\n",
        "      if not best_non_null_entry:\n",
        "        if entry.text:\n",
        "          best_non_null_entry = entry\n",
        "\n",
        "    probs = run_squad._compute_softmax(total_scores)\n",
        "\n",
        "    nbest_json = []\n",
        "    for (i, entry) in enumerate(nbest):\n",
        "      output = collections.OrderedDict()\n",
        "      output[\"text\"] = entry.text\n",
        "      output[\"probability\"] = probs[i]\n",
        "      output[\"start_logit\"] = entry.start_logit\n",
        "      output[\"end_logit\"] = entry.end_logit\n",
        "      nbest_json.append(output)\n",
        "\n",
        "    assert len(nbest_json) >= 1\n",
        "\n",
        "    all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
        "\n",
        "    all_nbest_json[example.qas_id] = nbest_json\n",
        "\n",
        "  with tf.gfile.GFile(output_prediction_file, \"w\") as writer:\n",
        "    writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
        "\n",
        "  with tf.gfile.GFile(output_nbest_file, \"w\") as writer:\n",
        "    writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
        "    \n",
        "\n",
        "def _get_best_indexes(logits, n_best_size):\n",
        "  \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "  index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  best_indexes = []\n",
        "  for i in range(len(index_and_score)):\n",
        "    if i >= n_best_size:\n",
        "      break\n",
        "    best_indexes.append(index_and_score[i][0])\n",
        "  return best_indexes"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emo7E3__bwCU"
      },
      "source": [
        "import collections\n",
        "import json\n",
        "\n",
        "write_predictions(eval_examples, eval_features, all_results,\n",
        "                      20, 30,\n",
        "                      True, output_prediction_file,\n",
        "                      output_nbest_file, output_null_log_odds_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "O_TNH4FjbyS4",
        "outputId": "ca1fbcc0-adfe-445c-90ac-b04a0f544bc9"
      },
      "source": [
        "train_df_full[train_df_full.ID == \"development-67\"]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Text</th>\n",
              "      <th>Pronoun</th>\n",
              "      <th>Pronoun-offset</th>\n",
              "      <th>A</th>\n",
              "      <th>A-offset</th>\n",
              "      <th>A-coref</th>\n",
              "      <th>B</th>\n",
              "      <th>B-offset</th>\n",
              "      <th>B-coref</th>\n",
              "      <th>URL</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>doc_tokens</th>\n",
              "      <th>char_to_word_offset</th>\n",
              "      <th>orig_answer_text</th>\n",
              "      <th>start_position</th>\n",
              "      <th>end_position</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>development-67</td>\n",
              "      <td>After it is revealed to Misty that Tabitha is ...</td>\n",
              "      <td>her</td>\n",
              "      <td>269</td>\n",
              "      <td>Grace</td>\n",
              "      <td>187</td>\n",
              "      <td>True</td>\n",
              "      <td>Tabitha</td>\n",
              "      <td>234</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Diary_(novel)</td>\n",
              "      <td>False</td>\n",
              "      <td>[After, it, is, revealed, to, Misty, that, Tab...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, ...</td>\n",
              "      <td>Grace</td>\n",
              "      <td>33</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                ID  ... end_position\n",
              "66  development-67  ...           33\n",
              "\n",
              "[1 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YKJPIHeb1sN",
        "outputId": "4defa45e-2fa4-41d3-9df3-b32e1b07c55a"
      },
      "source": [
        "!cat model_repo/outputs/predictions.json"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"development-67\": \"Grace\",\n",
            "    \"development-883\": \"Soulja Boy\",\n",
            "    \"development-1144\": \"Lynn\",\n",
            "    \"development-253\": \"Celia\",\n",
            "    \"development-814\": \"Toskala\",\n",
            "    \"development-1400\": \"Narcizo\",\n",
            "    \"development-423\": \"Gower\",\n",
            "    \"development-92\": \"Palmer\",\n",
            "    \"development-371\": \"Catherine Marshall\",\n",
            "    \"development-1120\": \"Marie\",\n",
            "    \"development-1385\": \"Devoy\",\n",
            "    \"development-1995\": \"Jennifer Hudson\",\n",
            "    \"development-1777\": \"Mary Geary\",\n",
            "    \"development-1526\": \"Clerck\",\n",
            "    \"development-915\": \"Friederike\",\n",
            "    \"development-1299\": \"Jakac\",\n",
            "    \"development-1739\": \"Alexandra Byrne\",\n",
            "    \"development-1420\": \"Laming\",\n",
            "    \"development-587\": \"Leadbetter\",\n",
            "    \"development-1364\": \"Breuer had a strong talent and turned Williamson\",\n",
            "    \"development-819\": \"Natalie\",\n",
            "    \"development-1303\": \"Havard\",\n",
            "    \"development-834\": \"Top Cat\",\n",
            "    \"development-241\": \"Cilla\",\n",
            "    \"development-1275\": \"JJ Babu\",\n",
            "    \"development-1708\": \"Catherine\",\n",
            "    \"development-1064\": \"Krenzel\",\n",
            "    \"development-973\": \"Ernestam\",\n",
            "    \"development-313\": \"Vickie Guerrero\",\n",
            "    \"development-297\": \"John Wayne\",\n",
            "    \"development-972\": \"David Hunt\",\n",
            "    \"development-339\": \"Ibn Thabit\",\n",
            "    \"development-1706\": \"Marie\",\n",
            "    \"development-697\": \"Bernini\",\n",
            "    \"development-1952\": \"Lily Allen\",\n",
            "    \"development-1666\": \"Maria Scharnetzki\",\n",
            "    \"development-438\": \"Medbouh discovered the reason : William P. Rogers\",\n",
            "    \"development-888\": \"Laura Elizabeth Pedersen\",\n",
            "    \"development-839\": \"Machi\",\n",
            "    \"development-1775\": \"Oller\",\n",
            "    \"development-1141\": \"Kane Cleal\",\n",
            "    \"development-3\": \"De la Sota\",\n",
            "    \"development-1765\": \"Isabelle Grossman\",\n",
            "    \"development-1951\": \"Ann\",\n",
            "    \"development-1302\": \"Reeves\",\n",
            "    \"development-500\": \"John Hoskins\",\n",
            "    \"development-263\": \"Bordo\",\n",
            "    \"development-479\": \"John Wadsworth\",\n",
            "    \"development-1934\": \"Gwen\",\n",
            "    \"development-1896\": \"Claudia Kulichevskaya\",\n",
            "    \"development-450\": \"Melora\",\n",
            "    \"development-710\": \"Jessica\",\n",
            "    \"development-1028\": \"Kaori\",\n",
            "    \"development-525\": \"Steve Zissou ( Bill Murray\",\n",
            "    \"development-656\": \"Johan van Dorth\",\n",
            "    \"development-1453\": \"Tolsma returned to the team in 1997, running a limited schedule for Coulter\",\n",
            "    \"development-1238\": \"Stephanie\",\n",
            "    \"development-1223\": \"Adams\",\n",
            "    \"development-1011\": \"Chambers received a call up to the Republic of Ireland U21 squad by manager Don Givens. Chambers\",\n",
            "    \"development-1799\": \"Peters\",\n",
            "    \"development-1697\": \"Michelle\",\n",
            "    \"development-891\": \"Bass\",\n",
            "    \"development-660\": \"Karen\",\n",
            "    \"development-1837\": \"Nancy\",\n",
            "    \"development-173\": \"Murray\",\n",
            "    \"development-202\": \"Artigas\",\n",
            "    \"development-217\": \"George H. Throop\",\n",
            "    \"development-981\": \"Graves\",\n",
            "    \"development-1045\": \"Perry\",\n",
            "    \"development-329\": \"Constantine's mother entered into negotiations with the Byzantine Empire and, in exchange for recognizing nominal Byzantine suzerainty, Constantine\",\n",
            "    \"development-1345\": \"Sally\",\n",
            "    \"development-663\": \"Conor Whately\",\n",
            "    \"development-590\": \"Maria Roscoe\",\n",
            "    \"development-1269\": \"Landrith\",\n",
            "    \"development-776\": \"Daniell\",\n",
            "    \"development-498\": \"Gvinjia\",\n",
            "    \"development-1078\": \"Williams\",\n",
            "    \"development-376\": \"Englefield\",\n",
            "    \"development-1730\": \"Jo Pavey\",\n",
            "    \"development-719\": \"Angela\",\n",
            "    \"development-569\": \"Koivu\",\n",
            "    \"development-456\": \"Marcus Cicero\",\n",
            "    \"development-1082\": \"Marina Yakhlakova\",\n",
            "    \"development-1491\": \"Chris Steinfort\",\n",
            "    \"development-982\": \"Gomi\",\n",
            "    \"development-1328\": \"Shea\",\n",
            "    \"development-1292\": \"Huntsman\",\n",
            "    \"development-69\": \"Margaret\",\n",
            "    \"development-147\": \"M'Naghten\",\n",
            "    \"development-1534\": \"God\",\n",
            "    \"development-884\": \"Ted Schwinden\",\n",
            "    \"development-1687\": \"Rimes\",\n",
            "    \"development-1897\": \"Volosozhar\",\n",
            "    \"development-493\": \"Amanda\",\n",
            "    \"development-355\": \"Thomas\",\n",
            "    \"development-1074\": \"Jane\",\n",
            "    \"development-1033\": \"Suhasini\",\n",
            "    \"development-1352\": \"Wurtzel\",\n",
            "    \"development-1981\": \"Anne Sewell Young\",\n",
            "    \"development-278\": \"Tad Friend\",\n",
            "    \"development-268\": \"Drum\",\n",
            "    \"development-1241\": \"David Larible\",\n",
            "    \"development-255\": \"Zelena\",\n",
            "    \"development-54\": \"Lorenzo Rubini\",\n",
            "    \"development-1186\": \"Derval\",\n",
            "    \"development-1261\": \"William Harley\",\n",
            "    \"development-106\": \"Streep\",\n",
            "    \"development-1570\": \"Sincerity Travers\",\n",
            "    \"development-1804\": \"Elizabeth Ryves\",\n",
            "    \"development-706\": \"Sarah Armantrout, a longtime friend who was with White\",\n",
            "    \"development-526\": \"Bruton\",\n",
            "    \"development-556\": \"Rosa Mendes\",\n",
            "    \"development-1242\": \"Ra Chuda\",\n",
            "    \"development-1160\": \"Cathal\",\n",
            "    \"development-271\": \"Rana Pratap\",\n",
            "    \"development-1143\": \"Lynn\",\n",
            "    \"development-1830\": \"Mary\",\n",
            "    \"development-936\": \"Bloodworth\",\n",
            "    \"development-144\": \"Diane\",\n",
            "    \"development-566\": \"Ragnar\",\n",
            "    \"development-624\": \"Bol*var\",\n",
            "    \"development-1802\": \"Shana\",\n",
            "    \"development-1752\": \"Cinoman\",\n",
            "    \"development-1535\": \"Josephine\",\n",
            "    \"development-1674\": \"Jenny\",\n",
            "    \"development-467\": \"Pius II\",\n",
            "    \"development-1274\": \"Dickey\",\n",
            "    \"development-628\": \"Colonel Finch\",\n",
            "    \"development-1103\": \"Ronaldinho\",\n",
            "    \"development-1182\": \"Seeta\",\n",
            "    \"development-1310\": \"Ottmar Hitzfeld\",\n",
            "    \"development-808\": \"Kylie\",\n",
            "    \"development-1040\": \"Cousteau\",\n",
            "    \"development-1007\": \"Chimbudevan\",\n",
            "    \"development-1278\": \"L*wenthal\",\n",
            "    \"development-1953\": \"Adele\",\n",
            "    \"development-259\": \"Malcolm Rifkind\",\n",
            "    \"development-477\": \"Frei\",\n",
            "    \"development-1128\": \"Matthews\",\n",
            "    \"development-718\": \"Fleming\",\n",
            "    \"development-1756\": \"Maxie\",\n",
            "    \"development-497\": \"Ward\",\n",
            "    \"development-1035\": \"Neville\",\n",
            "    \"development-1862\": \"Geena Davis\",\n",
            "    \"development-803\": \"Hirsi Ali\",\n",
            "    \"development-1623\": \"Cindy\",\n",
            "    \"development-1984\": \"Lady Elizabeth\",\n",
            "    \"development-51\": \"Bo-lung\",\n",
            "    \"development-227\": \"Wood\",\n",
            "    \"development-1429\": \"Dumanis\",\n",
            "    \"development-796\": \"Edward Baigent\",\n",
            "    \"development-1176\": \"Donaldson\",\n",
            "    \"development-494\": \"Jeremy\",\n",
            "    \"development-381\": \"Justin Tranter\",\n",
            "    \"development-838\": \"Miu\",\n",
            "    \"development-1044\": \"Walker\",\n",
            "    \"development-185\": \"Parol\",\n",
            "    \"development-678\": \"David Freiburger\",\n",
            "    \"development-234\": \"Charismatic\",\n",
            "    \"development-823\": \"Victor Jara\",\n",
            "    \"development-1484\": \"Sarah Jane\",\n",
            "    \"development-505\": \"Babu Veer Kunwar Singh\",\n",
            "    \"development-847\": \"Stephenson\",\n",
            "    \"development-327\": \"Larsson\",\n",
            "    \"development-312\": \"Harriet\",\n",
            "    \"development-745\": \"Perot\",\n",
            "    \"development-844\": \"Mary\",\n",
            "    \"development-128\": \"Smith\",\n",
            "    \"development-1631\": \"John Rist\",\n",
            "    \"development-4\": \"Rank\",\n",
            "    \"development-1243\": \"Saier\",\n",
            "    \"development-281\": \"Eleanor\",\n",
            "    \"development-1177\": \"Bobby\",\n",
            "    \"development-916\": \"Cleavon\",\n",
            "    \"development-1523\": \"Ann Lovell Gwatkin\",\n",
            "    \"development-931\": \"Dempsey\",\n",
            "    \"development-115\": \"Taffy\",\n",
            "    \"development-692\": \"Ethan\",\n",
            "    \"development-464\": \"Ed\",\n",
            "    \"development-1175\": \"Gilbert de Clare\",\n",
            "    \"development-1024\": \"Magnus Forteman\",\n",
            "    \"development-1912\": \"Clack\",\n",
            "    \"development-1646\": \"Pratt\",\n",
            "    \"development-1889\": \"Johnny\",\n",
            "    \"development-852\": \"Goebbels\",\n",
            "    \"development-1903\": \"Holly Hathorn\",\n",
            "    \"development-557\": \"Gardiner\",\n",
            "    \"development-1265\": \"Britta\",\n",
            "    \"development-470\": \"Louise\",\n",
            "    \"development-514\": \"Lemmons\",\n",
            "    \"development-1668\": \"Christine Charlotte\",\n",
            "    \"development-516\": \"Steverson\",\n",
            "    \"development-523\": \"Jay-Z\",\n",
            "    \"development-301\": \"Bracewell\",\n",
            "    \"development-124\": \"Sheikh Pahar\",\n",
            "    \"development-1682\": \"Linda Rogoff\",\n",
            "    \"development-879\": \"Loew\",\n",
            "    \"development-360\": \"Harris\",\n",
            "    \"development-1381\": \"Levitan, then 14 years old, tracked Lennon\",\n",
            "    \"development-422\": \"Thoms\",\n",
            "    \"development-730\": \"Joel\",\n",
            "    \"development-1109\": \"Odin\",\n",
            "    \"development-305\": \"Susan\",\n",
            "    \"development-1076\": \"Singh\",\n",
            "    \"development-89\": \"Giuseppe Verdi\",\n",
            "    \"development-806\": \"Elizabeth Barrett\",\n",
            "    \"development-1216\": \"Wilhelmina\",\n",
            "    \"development-324\": \"Candace\",\n",
            "    \"development-649\": \"Angela\",\n",
            "    \"development-1038\": \"Chandran\",\n",
            "    \"development-771\": \"Reginald Dallas Brooks\",\n",
            "    \"development-1878\": \"Marrero\",\n",
            "    \"development-990\": \"Cynthia Brooke\",\n",
            "    \"development-1977\": \"Amy\",\n",
            "    \"development-1129\": \"Anthony O'Donnell\",\n",
            "    \"development-788\": \"Chen\",\n",
            "    \"development-613\": \"Hayes\",\n",
            "    \"development-440\": \"Panizza\",\n",
            "    \"development-546\": \"Helen\",\n",
            "    \"development-1393\": \"Charlotte notices Colin\",\n",
            "    \"development-1816\": \"Eloise\",\n",
            "    \"development-72\": \"Ney\",\n",
            "    \"development-1590\": \"Hughes\",\n",
            "    \"development-242\": \"Patrick\",\n",
            "    \"development-1068\": \"Jack\",\n",
            "    \"development-703\": \"Clay\",\n",
            "    \"development-935\": \"Keyes\",\n",
            "    \"development-1499\": \"Squire Bancroft\",\n",
            "    \"development-59\": \"Frank\",\n",
            "    \"development-887\": \"Stephanie\",\n",
            "    \"development-1542\": \"Manapat\",\n",
            "    \"development-552\": \"Baynes\",\n",
            "    \"development-928\": \"Aikman\",\n",
            "    \"development-1989\": \"Lily\",\n",
            "    \"development-1477\": \"Jerry Holland\",\n",
            "    \"development-471\": \"Polly\",\n",
            "    \"development-1883\": \"Bertha Benz\",\n",
            "    \"development-1841\": \"Bette Midler\",\n",
            "    \"development-1598\": \"Britton\",\n",
            "    \"development-1341\": \"Malcolm\",\n",
            "    \"development-727\": \"Seles\",\n",
            "    \"development-1749\": \"Deanna\",\n",
            "    \"development-1354\": \"Averell\",\n",
            "    \"development-1013\": \"Sandoval\",\n",
            "    \"development-713\": \"Sarah Palin\",\n",
            "    \"development-863\": \"William Ospina\",\n",
            "    \"development-618\": \"Grisi\",\n",
            "    \"development-1877\": \"Claudia Card\",\n",
            "    \"development-1824\": \"Elizabeth Goudie Story'', starring Sherry Smith\",\n",
            "    \"development-1366\": \"Sam Spade\",\n",
            "    \"development-661\": \"Cox\",\n",
            "    \"development-236\": \"Isabel\",\n",
            "    \"development-200\": \"Belle\",\n",
            "    \"development-869\": \"William Livingston\",\n",
            "    \"development-100\": \"Palmer\",\n",
            "    \"development-1684\": \"Natalie Appleton\",\n",
            "    \"development-903\": \"Sara\",\n",
            "    \"development-1203\": \"Matthew Vaughn\",\n",
            "    \"development-701\": \"Thomas\",\n",
            "    \"development-1894\": \"Kurup\",\n",
            "    \"development-728\": \"Kate Flannery\",\n",
            "    \"development-475\": \"Carr\",\n",
            "    \"development-1360\": \"Mroz\",\n",
            "    \"development-20\": \"Ramsey\",\n",
            "    \"development-1215\": \"Betty\",\n",
            "    \"development-1891\": \"Carlotta\",\n",
            "    \"development-1954\": \"Joanne\",\n",
            "    \"development-1567\": \"Stibor\",\n",
            "    \"development-1402\": \"Andy\",\n",
            "    \"development-1924\": \"Dorothy Gael\",\n",
            "    \"development-1397\": \"Kleinow\",\n",
            "    \"development-1054\": \"Wilhelmina Vautrin\",\n",
            "    \"development-1854\": \"Audrey\",\n",
            "    \"development-333\": \"Wong Fei Hung\",\n",
            "    \"development-743\": \"Ethel Kennedy\",\n",
            "    \"development-247\": \"Yi'nan\",\n",
            "    \"development-1987\": \"Hannah\",\n",
            "    \"development-1181\": \"Johann von Wilderer\",\n",
            "    \"development-1034\": \"Pat Quinn\",\n",
            "    \"development-917\": \"Cheung\",\n",
            "    \"development-1189\": \"Stephen\",\n",
            "    \"development-1065\": \"Fred Ordonez\",\n",
            "    \"development-53\": \"Henning\",\n",
            "    \"development-248\": \"David Cantwell\",\n",
            "    \"development-1703\": \"Marjaneh Moghimi\",\n",
            "    \"development-1234\": \"Veronica\",\n",
            "    \"development-201\": \"Macquarie\",\n",
            "    \"development-599\": \"Bighead\",\n",
            "    \"development-544\": \"Mary Katherine\",\n",
            "    \"development-1318\": \"Bohlander\",\n",
            "    \"development-1539\": \"Raju\",\n",
            "    \"development-466\": \"Robert Peary\",\n",
            "    \"development-468\": \"Jak\",\n",
            "    \"development-1764\": \"Sophie\",\n",
            "    \"development-183\": \"Edward I\",\n",
            "    \"development-822\": \"Carlos Bruce\",\n",
            "    \"development-965\": \"De Maris\",\n",
            "    \"development-1444\": \"Chou\",\n",
            "    \"development-923\": \"Quinn\",\n",
            "    \"development-554\": \"Tommy Stinson\",\n",
            "    \"development-856\": \"Owen\",\n",
            "    \"development-1372\": \"Mercer showed up ill prepared and Ferguson\",\n",
            "    \"development-1285\": \"Siri\",\n",
            "    \"development-878\": \"Zeki\",\n",
            "    \"development-9\": \"Kelsey\",\n",
            "    \"development-1929\": \"Alanis\",\n",
            "    \"development-1224\": \"Woods\",\n",
            "    \"development-1574\": \"Dryer\",\n",
            "    \"development-358\": \"Rose\",\n",
            "    \"development-1849\": \"Kerri Kenney\",\n",
            "    \"development-1839\": \"Speier\",\n",
            "    \"development-482\": \"Jones\",\n",
            "    \"development-35\": \"Bobby Peacock\",\n",
            "    \"development-1502\": \"Nixon\",\n",
            "    \"development-1865\": \"Clark\",\n",
            "    \"development-1714\": \"Richards\",\n",
            "    \"development-1089\": \"Doc\",\n",
            "    \"development-1264\": \"Abed\",\n",
            "    \"development-1098\": \"Chantal\",\n",
            "    \"development-927\": \"Thomas Penn\",\n",
            "    \"development-648\": \"John\",\n",
            "    \"development-732\": \"Carr\",\n",
            "    \"development-1810\": \"Shepherdson\",\n",
            "    \"development-1783\": \"Luna\",\n",
            "    \"development-1809\": \"Nicole Thorne\",\n",
            "    \"development-598\": \"Murasaki\",\n",
            "    \"development-425\": \"David Hopkins\",\n",
            "    \"development-1844\": \"Pilar is still very upset with Helena\",\n",
            "    \"development-1172\": \"Ellis\",\n",
            "    \"development-1169\": \"Jude\",\n",
            "    \"development-122\": \"Walter Kirn\",\n",
            "    \"development-1867\": \"Hattie\",\n",
            "    \"development-1885\": \"Vanitha\",\n",
            "    \"development-1329\": \"Domino\",\n",
            "    \"development-172\": \"Hayden\",\n",
            "    \"development-780\": \"Reid\",\n",
            "    \"development-1212\": \"Curtis\",\n",
            "    \"development-983\": \"Jonathon\",\n",
            "    \"development-1701\": \"Sarala\",\n",
            "    \"development-610\": \"Vald*tejn\",\n",
            "    \"development-1549\": \"Smith\",\n",
            "    \"development-1198\": \"Virginie\",\n",
            "    \"development-1072\": \"Makarov\",\n",
            "    \"development-629\": \"Woolf\",\n",
            "    \"development-1251\": \"Ron\",\n",
            "    \"development-1975\": \"Princess Elizabeth\",\n",
            "    \"development-378\": \"Temerlin\",\n",
            "    \"development-1359\": \"Porter\",\n",
            "    \"development-1917\": \"Lucy\",\n",
            "    \"development-1680\": \"Janet\",\n",
            "    \"development-1787\": \"Gurney\",\n",
            "    \"development-1497\": \"Rockov\",\n",
            "    \"development-1290\": \"Giuseppe Carelli (1858 - 1921), and uncle of Conrad Hector Raffaele Carelli\",\n",
            "    \"development-1692\": \"Zimmerman\",\n",
            "    \"development-272\": \"Jackson\",\n",
            "    \"development-359\": \"Paige\",\n",
            "    \"development-1556\": \"Lily\",\n",
            "    \"development-1298\": \"Garth\",\n",
            "    \"development-362\": \"Patricia\",\n",
            "    \"development-1955\": \"Ronnie\",\n",
            "    \"development-1336\": \"Heffron\",\n",
            "    \"development-457\": \"Dan Brown\",\n",
            "    \"development-605\": \"Vera Rockline\",\n",
            "    \"development-210\": \"Barlow\",\n",
            "    \"development-1218\": \"Nico\",\n",
            "    \"development-427\": \"Antonia\",\n",
            "    \"development-1793\": \"Alice\",\n",
            "    \"development-1423\": \"Childress\",\n",
            "    \"development-513\": \"Doyen\",\n",
            "    \"development-676\": \"Hiroyuki Nagahama\",\n",
            "    \"development-572\": \"John Benton\",\n",
            "    \"development-1774\": \"Sobotka\",\n",
            "    \"development-331\": \"Vivian\",\n",
            "    \"development-171\": \"Maria Theresa Miller\",\n",
            "    \"development-1496\": \"Susie\",\n",
            "    \"development-188\": \"Peter\",\n",
            "    \"development-32\": \"Fantaghir* and Esmeralda\",\n",
            "    \"development-716\": \"Tim\",\n",
            "    \"development-1747\": \"Mary Butler\",\n",
            "    \"development-165\": \"Soule\",\n",
            "    \"development-1319\": \"Alice\",\n",
            "    \"development-1228\": \"Heard\",\n",
            "    \"development-968\": \"Gina goes to see John who is entertaining Jill, he forces Gina\",\n",
            "    \"development-125\": \"John Turner\",\n",
            "    \"development-1517\": \"Colonel Greenup\",\n",
            "    \"development-1262\": \"Gombell\",\n",
            "    \"development-1171\": \"Virgil\",\n",
            "    \"development-1348\": \"Bernadette Soubirous\",\n",
            "    \"development-1904\": \"Sorcha Dallas\",\n",
            "    \"development-1503\": \"Roux\",\n",
            "    \"development-1595\": \"Cao Rui\",\n",
            "    \"development-1927\": \"Gordon\",\n",
            "    \"development-1937\": \"Phyllis Thede\",\n",
            "    \"development-642\": \"Anthony\",\n",
            "    \"development-481\": \"McCrimmon\",\n",
            "    \"development-1826\": \"Jill Bennett\",\n",
            "    \"development-921\": \"Kelly\",\n",
            "    \"development-1403\": \"Deller\",\n",
            "    \"development-1606\": \"Clarke\",\n",
            "    \"development-762\": \"Pedro\",\n",
            "    \"development-707\": \"Shelton\",\n",
            "    \"development-1185\": \"Christiana\",\n",
            "    \"development-585\": \"Catherine Donnelly\",\n",
            "    \"development-1942\": \"Megan\",\n",
            "    \"development-1616\": \"Heiberg\",\n",
            "    \"development-1939\": \"Sharapova\",\n",
            "    \"development-1808\": \"Donna\",\n",
            "    \"development-351\": \"Jo\",\n",
            "    \"development-361\": \"Kira Nerys\",\n",
            "    \"development-1573\": \"Claude Lalumi*re) is said to be Malignos (2000). He\",\n",
            "    \"development-277\": \"Bart\",\n",
            "    \"development-565\": \"Juber\",\n",
            "    \"development-524\": \"Brendon Hartley\",\n",
            "    \"development-1399\": \"Sara Karr\",\n",
            "    \"development-1146\": \"Ender Wiggin\",\n",
            "    \"development-537\": \"Grainger\",\n",
            "    \"development-146\": \"Rocky\",\n",
            "    \"development-1872\": \"Jemima Shore\",\n",
            "    \"development-1222\": \"duchess Henriette of Lorraine\",\n",
            "    \"development-1906\": \"Vani\",\n",
            "    \"development-932\": \"Spurlock\",\n",
            "    \"development-1121\": \"Boniface\",\n",
            "    \"development-894\": \"Yash that he is in love with Maya\",\n",
            "    \"development-574\": \"Andrew\",\n",
            "    \"development-837\": \"Courtney Gears\",\n",
            "    \"development-738\": \"Jeremy\",\n",
            "    \"development-731\": \"Andy Spade\",\n",
            "    \"development-1729\": \"Martha\",\n",
            "    \"development-1395\": \"Maria\",\n",
            "    \"development-1884\": \"Lily Elise (born February 20, 1991) is a singer, songwriter from Berkeley, California. Elise\",\n",
            "    \"development-1214\": \"Cavendish\",\n",
            "    \"development-408\": \"Frutkin was awarded the NASA Distinguished Service Medal by Thomas Paine, and Frutkin\",\n",
            "    \"development-1565\": \"Jervis\",\n",
            "    \"development-1208\": \"Stewart\",\n",
            "    \"development-332\": \"Victor\",\n",
            "    \"development-209\": \"Giggs\",\n",
            "    \"development-1156\": \"Pallo\",\n",
            "    \"development-1712\": \"Nikki\",\n",
            "    \"development-922\": \"Willard\",\n",
            "    \"development-1488\": \"Peter Cetera\",\n",
            "    \"development-276\": \"Lisa\",\n",
            "    \"development-1993\": \"Jenny\",\n",
            "    \"development-724\": \"Kristian Zahrtmann\",\n",
            "    \"development-602\": \"Graham\",\n",
            "    \"development-1642\": \"Elena\",\n",
            "    \"development-799\": \"Lembeck\",\n",
            "    \"development-1124\": \"Catherine Ann Jones\",\n",
            "    \"development-413\": \"Roy Porter argues that Hackman\",\n",
            "    \"development-1492\": \"Charles Aperia\",\n",
            "    \"development-1472\": \"Cumming\",\n",
            "    \"development-1604\": \"Esme\",\n",
            "    \"development-520\": \"Robert Kubica was promoted internally at BMW to drive at the Hungaroring because Villeneuve\",\n",
            "    \"development-567\": \"Peter I\",\n",
            "    \"development-75\": \"Anstruther\",\n",
            "    \"development-1474\": \"Maggie\",\n",
            "    \"development-589\": \"Clark\",\n",
            "    \"development-1276\": \"Walter Brown\",\n",
            "    \"development-1723\": \"Rita\",\n",
            "    \"development-1554\": \"Dusio\",\n",
            "    \"development-644\": \"Lucas Greene\",\n",
            "    \"development-1585\": \"Claudia\",\n",
            "    \"development-238\": \"Edmund Fitzalan\",\n",
            "    \"development-679\": \"Arsenault\",\n",
            "    \"development-1362\": \"Natasha\",\n",
            "    \"development-1373\": \"Felicia Scatcherd\",\n",
            "    \"development-334\": \"Dakota Fanning\",\n",
            "    \"development-1773\": \"Jarosz\",\n",
            "    \"development-1569\": \"Brock\",\n",
            "    \"development-1843\": \"Paget\",\n",
            "    \"development-579\": \"Alf\",\n",
            "    \"development-944\": \"Nick Mileti\",\n",
            "    \"development-843\": \"Lee\",\n",
            "    \"development-166\": \"Archbishop Ussher\",\n",
            "    \"development-239\": \"Crummell\",\n",
            "    \"development-114\": \"Richthofen\",\n",
            "    \"development-250\": \"Conrad\",\n",
            "    \"development-754\": \"Lady Guo\",\n",
            "    \"development-1812\": \"Althea\",\n",
            "    \"development-1890\": \"Lyja\",\n",
            "    \"development-1134\": \"Hedger\",\n",
            "    \"development-472\": \"Richter\",\n",
            "    \"development-283\": \"Bissett\",\n",
            "    \"development-206\": \"Blige\",\n",
            "    \"development-553\": \"Robins\",\n",
            "    \"development-1621\": \"Cheryl then grew up hating and resenting Marlene\",\n",
            "    \"development-41\": \"Karl Philipp\",\n",
            "    \"development-1012\": \"William\",\n",
            "    \"development-955\": \"Jordanus\",\n",
            "    \"development-612\": \"Park\",\n",
            "    \"development-1990\": \"Musgrave\",\n",
            "    \"development-699\": \"Sarakalo\",\n",
            "    \"development-1599\": \"Nancy\",\n",
            "    \"development-840\": \"Charles\",\n",
            "    \"development-1363\": \"Richard\",\n",
            "    \"development-73\": \"Pinter\",\n",
            "    \"development-347\": \"Gerard\",\n",
            "    \"development-950\": \"Boxx\",\n",
            "    \"development-1515\": \"Compton\",\n",
            "    \"development-1945\": \"Julia Anne\",\n",
            "    \"development-1695\": \"Peace\",\n",
            "    \"development-761\": \"Martha\",\n",
            "    \"development-461\": \"Alec Bedser\",\n",
            "    \"development-1259\": \"Claire Wellington\",\n",
            "    \"development-1694\": \"Kurys\",\n",
            "    \"development-1979\": \"Jacky\",\n",
            "    \"development-1312\": \"Omlie\",\n",
            "    \"development-809\": \"Edmund Calvert\",\n",
            "    \"development-1914\": \"Anne\",\n",
            "    \"development-1738\": \"Robertson\",\n",
            "    \"development-857\": \"Wilson\",\n",
            "    \"development-1930\": \"Stephenson\",\n",
            "    \"development-1946\": \"Eddowes and Kelly\",\n",
            "    \"development-1482\": \"Shaina\",\n",
            "    \"development-447\": \"Thomas\",\n",
            "    \"development-1467\": \"Fujisawa\",\n",
            "    \"development-608\": \"Cate Blanchett\",\n",
            "    \"development-1768\": \"Chelsea\",\n",
            "    \"development-691\": \"Robert Ricard\",\n",
            "    \"development-850\": \"Dinah\",\n",
            "    \"development-570\": \"Roberts\",\n",
            "    \"development-1315\": \"Grimwade\",\n",
            "    \"development-1649\": \"Teresa\",\n",
            "    \"development-704\": \"Marco\",\n",
            "    \"development-1398\": \"Larkin\",\n",
            "    \"development-159\": \"John Deacon\",\n",
            "    \"development-994\": \"Smriti\",\n",
            "    \"development-1244\": \"Grace Lichtenstein\",\n",
            "    \"development-542\": \"JESS3 has undertaken with Thomas\",\n",
            "    \"development-600\": \"Pagett\",\n",
            "    \"development-1529\": \"Constance\",\n",
            "    \"development-430\": \"Gale Page\",\n",
            "    \"development-1091\": \"Coulton\",\n",
            "    \"development-163\": \"Riotta\",\n",
            "    \"development-380\": \"Julius Constantius\",\n",
            "    \"development-325\": \"Umfraville\",\n",
            "    \"development-609\": \"Randolf Menzel\",\n",
            "    \"development-1480\": \"Walsh\",\n",
            "    \"development-635\": \"Emich I\",\n",
            "    \"development-269\": \"McClintock\",\n",
            "    \"development-1900\": \"Kimberley Santos\",\n",
            "    \"development-1857\": \"Johnson\",\n",
            "    \"development-945\": \"Bakekang\",\n",
            "    \"development-1339\": \"Cable\",\n",
            "    \"development-344\": \"Chueathai\",\n",
            "    \"development-899\": \"Martha Washington\",\n",
            "    \"development-1325\": \"Sylvia Mary Montgomery\",\n",
            "    \"development-1209\": \"Bailey\",\n",
            "    \"development-1107\": \"Arch Hall\",\n",
            "    \"development-1473\": \"Diana Mitford\",\n",
            "    \"development-964\": \"Fletcher\",\n",
            "    \"development-437\": \"Alec Finn\",\n",
            "    \"development-1731\": \"Golonka\",\n",
            "    \"development-1524\": \"Harriet\",\n",
            "    \"development-1039\": \"Jun Iwasaki\",\n",
            "    \"development-1700\": \"Rose Mofford\",\n",
            "    \"development-573\": \"Joseph\",\n",
            "    \"development-1776\": \"Jane\",\n",
            "    \"development-580\": \"Evans\",\n",
            "    \"development-1219\": \"King Robert the Bruce\",\n",
            "    \"development-717\": \"G.K. Moopanar\",\n",
            "    \"development-1909\": \"Glasscock\",\n",
            "    \"development-969\": \"Needles\",\n",
            "    \"development-864\": \"Dan\",\n",
            "    \"development-1784\": \"Madonna\",\n",
            "    \"development-321\": \"Villanueva\",\n",
            "    \"development-1493\": \"Palmore\",\n",
            "    \"development-1636\": \"Anjali\",\n",
            "    \"development-295\": \"Seung\",\n",
            "    \"development-214\": \"Mickey\",\n",
            "    \"development-1596\": \"Reben\",\n",
            "    \"development-1613\": \"Ussher\",\n",
            "    \"development-1396\": \"Nikephoros\",\n",
            "    \"development-1459\": \"Binder\",\n",
            "    \"development-652\": \"Brown\",\n",
            "    \"development-1284\": \"Big Kenny\",\n",
            "    \"development-33\": \"Christine\",\n",
            "    \"development-219\": \"Toshney\",\n",
            "    \"development-211\": \"Brown\",\n",
            "    \"development-1817\": \"Louise\",\n",
            "    \"development-705\": \"Simonetta\",\n",
            "    \"development-221\": \"Jessen\",\n",
            "    \"development-1119\": \"Tom\",\n",
            "    \"development-1825\": \"Throsby\",\n",
            "    \"development-16\": \"McCulloch\",\n",
            "    \"development-1828\": \"Ryan\",\n",
            "    \"development-757\": \"Garrett\",\n",
            "    \"development-1384\": \"Giovanni Franzoni\",\n",
            "    \"development-802\": \"Humayun\",\n",
            "    \"development-107\": \"Asha\",\n",
            "    \"development-1320\": \"Biwott\",\n",
            "    \"development-1407\": \"Wood\",\n",
            "    \"development-1815\": \"Vivienne\",\n",
            "    \"development-632\": \"Adam\",\n",
            "    \"development-1511\": \"Aurelius\",\n",
            "    \"development-1184\": \"Castle\",\n",
            "    \"development-1655\": \"Olga Alexandrovna Girya\",\n",
            "    \"development-157\": \"Liszt\",\n",
            "    \"development-1085\": \"Hugues\",\n",
            "    \"development-42\": \"Hanbury\",\n",
            "    \"development-1543\": \"Timbilil\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj0Ybh6Kb_0B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}